# ðŸ“Š ETL Data Pipeline

Proyecto de **IngenierÃ­a de Datos** con un pipeline ETL (Extract, Transform, Load).  
Los datos se extraen de una API pÃºblica, se transforman con **pandas** y se cargan en **PostgreSQL**.

# Para ejecutar:
- configura tu base de datos (PostgreSQL):
`CREATE DATABASE etl_db;
CREATE USER etl_user WITH ENCRYPTED PASSWORD 'etl_password';
GRANT ALL PRIVILEGES ON DATABASE etl_db TO etl_user;`

- Para ejecutar el script, usa `python run_pipeline.py`.

# ðŸš€ TecnologÃ­as
- Python
- pandas
- PostgreSQL
- Apache Airflow / Prefect `proximamente`
- Docker

## ðŸ“‚ Estructura
- `extract/` â†’ scripts de extracciÃ³n de datos.
- `transform/` â†’ limpieza y normalizaciÃ³n.
- `load/` â†’ carga a la base de datos.
- `dags/` â†’ flujos de Airflow.

# Porque opte por esta estructura?
- Modular: cada etapa ETL es independiente â†’ fÃ¡cil mantenimiento.
- Preparada para migrar a Airflow: despuÃ©s podÃ©s convertir cada script en una tarea (PythonOperator).

## ðŸ’¡ PrÃ³ximos pasos
- Agregar orquestaciÃ³n con Airflow.
- Conectar con dashboard de visualizaciÃ³n.
